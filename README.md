# DQN 2048游戏训练项目

这是一个使用深度Q网络(DQN)训练智能体来玩2048游戏的强化学习项目。项目支持动作掩码、K步前瞻搜索等高级特性。

## 项目结构

```
FINAL/
├── run.py                 # 主入口文件
├── config.py              # 配置文件
├── README.md              # 项目说明
├── __init__.py            # 项目初始化文件
├── models/                # 模型模块
│   ├── __init__.py
│   └── dqn_network.py     # DQN网络结构
├── agents/                # 智能体模块
│   ├── __init__.py
│   └── dqn_agent.py       # DQN智能体实现
├── utils/                 # 工具模块
│   ├── __init__.py
│   ├── replay_buffer.py   # 经验回放缓冲区
│   └── visualization.py   # 可视化工具
└── game/                  # 游戏模块
    ├── __init__.py
    └── game_2048.py       # 2048游戏逻辑
```

## 功能特性

- **动作掩码**: 只选择有效动作，提高学习效率
- **K步前瞻搜索**: 在测试时使用蒙特卡洛树搜索提高性能
- **Double DQN**: 减少Q值过高估计的问题
- **软更新**: 平滑更新目标网络
- **增强奖励函数**: 考虑空位、最大瓦片、角落位置等因素
- **动态探索策略**: 根据训练阶段调整探索程度

## 安装依赖

```bash
pip install torch gymnasium gymnasium-2048 numpy matplotlib
```

## 使用方法

### 1. 训练模型

```bash
python run.py train
```

### 2. 测试模型

```bash
# 测试指定模型
python run.py test models_discrete/model.pth

# 例子
# 测试时可以调整config.py中    
    #K_STEP = 0  
    #NUM_SIMULATIONS = 0  
#暂时禁用测试时k步前瞻，加快测试时的推理速度
python run.py test models_discrete/dqn_valid_mask_gymnasium_2048_TwentyFortyEight-v0_20251228_021959_ep990.pth
```

模型测试之后会在2048_logs文件中生成一个json文件

### 3. 演示模型

```bash
#使用妙妙小工具重放2048_logs
python ManControl.py --replay 2048_logs/agent.json

#tip：可直接运行妙妙小工具以游玩游戏
python ManControl.py #游玩游戏，同时也会生成json文件以供后续重放
```
回放后会在2048_videos文件夹下生成视频文件


### 5. 查看帮助

```bash
python run.py help
```

## 配置参数

所有配置参数都在 `config.py` 文件中定义，包括：

- **训练参数**: 学习率、批次大小、训练回合数等
- **网络参数**: 隐藏层大小、初始化方法等
- **探索参数**: epsilon起始值、结束值、衰减率等
- **K步前瞻参数**: 搜索深度、模拟次数等
- **奖励函数参数**: 各项奖励的权重

## 输出文件

训练过程中会生成以下文件：

- `models_discrete/`: 保存训练的模型文件
- `2048_logs/`: 保存测试和评估日志
- `plots_discrete/`: 保存训练曲线图

## 性能指标

项目会记录以下性能指标：

- 每回合的奖励
- 最大瓦片值
- 达到2048的成功率
- 平均空位数量
- 训练损失

## 技术细节

### 网络结构

- **卷积神经网络架构**：使用2层卷积层处理4x4x16的one-hot编码输入
- **卷积层配置**：
  - 第一层：16通道输入，32通道输出，3x3卷积核，保持4x4尺寸
  - 第二层：32通道输入，64通道输出，3x3卷积核，输出2x2尺寸
- **全连接层**：64×2×2=256维展平后接64维隐藏层
- **激活函数**：ReLU
- **权重初始化**：Kaiming均匀分布，偏置初始化为0.01

### 数据增强

- **八重对称性利用**：通过旋转和翻转生成8倍数据增强
- **变换类型**：
  - 原始状态
  - 90°、180°、270°旋转
  - 水平翻转、垂直翻转
  - 主对角线转置、副对角线翻转
- **动作映射**：每种变换对应特定的动作映射关系
- **存储策略**：所有增强样本都存储到经验回放缓冲区

### 奖励函数设计

**增强奖励函数**（不依赖原始游戏奖励）：
```
R_enhanced = R_survival + R_emptiness + R_max_tile + R_monotonicity + R_corner + R_merge
```

- **生存奖励**：每步+1.0，鼓励持续游戏
- **空位奖励**：空位数×0.8，保持未来移动可能性
- **最大瓦片奖励**：log₂(最大瓦片)×0.5，对数奖励避免极端值
- **单调性奖励**：行列单调排列×0.2，鼓励有序布局
- **角落奖励**：最大瓦片≥64且在角落时+1.0
- **合并奖励**：log(1+原始奖励)×0.3，压缩合并奖励

### 动作掩码机制

- **有效性检查**：通过模拟移动确定每个动作的有效性
- **掩码应用**：将无效动作的Q值设为-1.0
- **训练优势**：避免学习无效动作，提高样本效率
- **实现方式**：Q值 = Q值 × 动作掩码 + (1-动作掩码) × (-1.0)

### K步前瞻搜索

**混合搜索策略**：
- **训练时**：浅搜索（K=2步，50次模拟）
- **测试时**：深搜索（K=5步，1000次模拟）
- **评估方式**：结合Q值（30%）和前瞻评估值（70%）
- **缓存机制**：缓存搜索结果避免重复计算
- **多步探索**：使用epsilon-贪婪选择前瞻中的动作

### 训练算法

- **经验回放**：缓冲区大小200,000，支持数据增强
- **目标网络更新**：软更新（τ=0.001）每100步执行一次
- **损失函数**：Huber损失，梯度裁剪（max_norm=0.5）
- **优化器**：Adam，学习率3e-4
- **探索策略**：动态epsilon衰减，根据训练阶段调整探索程度

### 动作选择策略

- **训练时**：
  - epsilon-贪婪策略，动态调整探索率
  - 三阶段学习：前期（20%）高探索，中期（60%）平衡，后期（20%）高利用
- **测试时**：
  - 确定性策略结合K步前瞻搜索
  - 对所有有效动作进行前瞻评估，选择最优动作

## 注意事项

1. 确保安装了所有必需的依赖包
2. 训练过程中会占用较多GPU内存
3. K步前瞻搜索会显著增加测试时间
4. 模型文件较大，注意磁盘空间

## 许可证

本项目仅供学习和研究使用。